{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraping(E-bay based).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfg30NeAErlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO\n",
        "# 1. Make a request to the necessary page and get a page\n",
        "# 2. Collect data from each detailed page\n",
        "# 3. Collect all links to detail pages of each product\n",
        "# 4. Write scrapped data to a csv file\n",
        "\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import csv\n",
        "import lxml\n",
        "\n",
        "# We need a function that will make requests to the necessary site. And let's define a new function and name it\n",
        "# get_page() that will take URL-address as an argument. And the get_page() function will make requests with the\n",
        "# Requests library, that we have installed.\n",
        "\n",
        "\n",
        "def get_page(url):                                     # requesting page from url source\n",
        "    response = requests.get(url)\n",
        "\n",
        "\n",
        "    if not response.ok:\n",
        "        print('Server responded: ', response.status_code)\n",
        "    else:\n",
        "        soup = BeautifulSoup(response.text, 'lxml')\n",
        "    return soup\n",
        "\n",
        "\n",
        "# That will contain a response of the site server. The 'response' variable is equal to the call of 'requests.get()'\n",
        "# method. And the '.get()' method takes the url variables as an argument.\n",
        "\n",
        "\n",
        "# So I'm creating a new function main() with the pass for a while..\n",
        "# the main function() will play the role of a hub, that will manage the calls of other functions and we'll\n",
        "# collect scraped data.\n",
        "\n",
        "\n",
        "def get_detailed_data(soup):\n",
        "    # title\n",
        "    # price\n",
        "    # items sold\n",
        "    try:\n",
        "\n",
        "        title = soup.find('h1', id='itemTitle').find('a').get('data-mtdes')\n",
        "    except:\n",
        "        title = ''                                                          # getting titles with the id\n",
        "    try:\n",
        "        try:\n",
        "            p = soup.find('span', id='prcIsum').text.strip().split(' ')     # original product id\n",
        "        except:\n",
        "            p = soup.find('span', id='mm-saleDscPrc').text.strip()         # discount product have a different id\n",
        "        currency, price = p.split(' ')            # splitting the currency and price\n",
        "    except:\n",
        "        currency = ''\n",
        "        price = ''\n",
        "\n",
        "    try:\n",
        "        sold = soup.find('span', class_='vi-qtysw-hot-red').find('a').text.strip().split(' ')[0].replace('\\xa0', '')  # getting sold number and cleaning\n",
        "    except:\n",
        "        sold = ''\n",
        "\n",
        "    data = {\n",
        "        'title': title,         # parsing the data library\n",
        "        'price': price,\n",
        "        'currency': currency,\n",
        "        'total sold': sold\n",
        "    }\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_index_data(soup):              # to get the pure link from colleted page\n",
        "    try:\n",
        "        links = soup.find_all('a', class_='s-item__link')\n",
        "\n",
        "    except:\n",
        "        links = []\n",
        "    urls = [item.get('href') for item in links]\n",
        "    return urls\n",
        "\n",
        "\n",
        "def write_csv(data, url):                                # writing data to csv file\n",
        "    with open('output.csv', 'a') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "\n",
        "        row = [data['title'], data['price'], data['currency'], data['total sold'], url]\n",
        "        writer.writerow(row)\n",
        "\n",
        "\n",
        "def main():\n",
        "    url = 'https://www.ebay.com/sch/i.html?_nkw=mens+watches&_pgn=1'\n",
        "\n",
        "    products = get_index_data(get_page(url))\n",
        "\n",
        "    for link in products:\n",
        "        data = get_detailed_data(get_page(link))\n",
        "        write_csv(data, link)\n",
        "\n",
        "\n",
        "# I suggest that we will scrape the needed data first and then we will add functionality to scrape all links to\n",
        "# inner pages. So we have a detailed page URL, that we have pass in to the 'get_page()' function and let's call the\n",
        "# get_page() function and pass in to it the URL variable.\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "# The IF condion checks whether the file 'Scraping(E-bay based)' was run directly from console or not.\n",
        "# If the file is running from the console its '__name__' attribute will be equal to '__main__' but if the file will be\n",
        "# imported to another script its '__name__' attribute will contain the name of the file 'Stock_Data_Scrapping'...\n",
        "# In this cae its name attribute will be equal to the 'Stock_Data_Scrapping'. And if this block returns \"True\" then the 'main()' function will be called.\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}